{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4a60afb3",
      "metadata": {
        "id": "4a60afb3"
      },
      "source": [
        "# EfficientNet-B3 Transfer Learning for ISIC 2019 Skin Lesion Classification\n",
        "\n",
        "## Medical Image Classification with Progressive Training\n",
        "\n",
        "This notebook implements EfficientNet-B3 transfer learning with progressive training for automated skin lesion diagnosis using the ISIC 2019 dataset.\n",
        "\n",
        "### Key Features:\n",
        "- **Progressive Training**: Two-stage approach (frozen backbone → full fine-tuning)\n",
        "- **EfficientNet-B3**: State-of-the-art efficient architecture optimized for medical images\n",
        "- **Enhanced Augmentation**: Medical-specific data augmentation pipeline\n",
        "- **Cosine Annealing**: Warm restart scheduler for improved convergence\n",
        "- **Class Balancing**: Handles imbalanced medical data\n",
        "\n",
        "### Architecture Overview:\n",
        "```\n",
        "Input (300x300) → EfficientNet-B3 Features → Global Avg Pool → Custom Classifier → 9 Classes\n",
        "                      ↓ Progressive Training ↓\n",
        "                 Stage 1: Frozen → Stage 2: Fine-tuned\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_CBioktuldlv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CBioktuldlv",
        "outputId": "7c3d47dd-c1fd-4c4a-b595-045fc8ceb559"
      },
      "outputs": [],
      "source": [
        "%cd /content/notebooks/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D0qIqjCOg1HU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0qIqjCOg1HU",
        "outputId": "96d1d563-fa67-4221-d13b-2599d3f93b25"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "print(f\"Current working directory: {os.getcwd()}\")\n",
        "\n",
        "scripts_dir = os.path.abspath('../scripts')\n",
        "print(f\"Attempting to add {scripts_dir} to sys.path\")\n",
        "sys.path.append(scripts_dir)\n",
        "\n",
        "try:\n",
        "    print(f\"Listing contents of {scripts_dir}:\")\n",
        "    if os.path.exists(scripts_dir):\n",
        "        print(os.listdir(scripts_dir))\n",
        "    else:\n",
        "        print(f\"Error: Directory not found at {scripts_dir}\")\n",
        "\n",
        "    # Attempt to import modules\n",
        "    from data_loader import AzureBlobLoader\n",
        "    from image_preprocessor import ImagePreprocessor\n",
        "    print(\"\\nSuccessfully imported data_loader and image_preprocessor!\")\n",
        "\n",
        "except ModuleNotFoundError as e:\n",
        "    print(f\"\\nImport Error: {e}\")\n",
        "    print(\"Please ensure the 'scripts' directory exists one level up from the notebook,\")\n",
        "    print(\"and that 'data_loader.py' and 'image_preprocessor.py' are present in that directory.\")\n",
        "except ImportError as e:\n",
        "    print(f\"\\nImport Error: {e}\")\n",
        "    print(\"There might be an issue within the data_loader.py or image_preprocessor.py files.\")\n",
        "    print(\"Please check the content of these script files for errors.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn unexpected error occurred during import: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af144f3a",
      "metadata": {
        "id": "af144f3a"
      },
      "source": [
        "## Setup and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "054e1297",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "054e1297",
        "outputId": "6eec1c48-6a8b-4e7a-efe6-1b9089248120"
      },
      "outputs": [],
      "source": [
        "# Essential imports\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Deep learning imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "from torchvision import transforms, models\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Add scripts directory to path\n",
        "sys.path.append('../scripts')\n",
        "from data_loader import AzureBlobLoader\n",
        "from image_preprocessor import ImagePreprocessor\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "print(\"Setup complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "003385ed",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "003385ed",
        "outputId": "37fb24be-8a7e-4ce5-ad49-ee21effe2609"
      },
      "outputs": [],
      "source": [
        "# # Configuration constants\n",
        "# CONFIG = {\n",
        "#     'azure_account': 'w281saysxxfypm',\n",
        "#     'azure_container': 'isic-2019-data',\n",
        "#     'data_dir': '../data',\n",
        "#     'images_dir': '../data/isic_images',\n",
        "#     'metadata_file': 'ISIC_2019_Training_Metadata.csv',\n",
        "#     'ground_truth_file': 'ISIC_2019_Training_GroundTruth.csv',\n",
        "#     'num_images': 5000,  # For initial testing\n",
        "#     'batch_size': 32,\n",
        "#     'num_epochs': 50,\n",
        "#     'learning_rate': 0.001,\n",
        "#     'weight_decay': 1e-4,\n",
        "#     'num_workers': 4,\n",
        "#     'patience': 10,  # Early stopping\n",
        "#     'validation_split': 0.2,\n",
        "#     'test_split': 0.1,\n",
        "#     'image_size': 224,  # VGG16 input size\n",
        "#     'num_classes': 9,\n",
        "#     'class_names': ['MEL', 'NV', 'BCC', 'AK', 'BKL', 'DF', 'VASC', 'SCC', 'UNK']\n",
        "# }\n",
        "\n",
        "# print(\"Configuration loaded:\")\n",
        "# for key, value in CONFIG.items():\n",
        "#     print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03a3bc1e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Updated configuration for EfficientNet-B3 with progressive training\n",
        "CONFIG = {\n",
        "    'azure_account': 'w281saysxxfypm',\n",
        "    'azure_container': 'isic-2019-data',\n",
        "    'data_dir': '../data',\n",
        "    'images_dir': '../data/isic_images',\n",
        "    'metadata_file': 'ISIC_2019_Training_Metadata.csv',\n",
        "    'ground_truth_file': 'ISIC_2019_Training_GroundTruth.csv',\n",
        "    'num_images': 8000,  # Increased for better EfficientNet performance\n",
        "    'batch_size': 16,    # Reduced for EfficientNet-B3 memory requirements\n",
        "    'num_epochs': 60,    # Increased for progressive training\n",
        "    'learning_rate': 5e-4,  # Lower learning rate for EfficientNet\n",
        "    'weight_decay': 1e-5,   # Reduced weight decay\n",
        "    'num_workers': 4,\n",
        "    'patience': 15,      # Increased patience for progressive training\n",
        "    'validation_split': 0.2,\n",
        "    'test_split': 0.1,\n",
        "    'image_size': 300,   # EfficientNet-B3 native size (can also use 224)\n",
        "    'num_classes': 9,\n",
        "    'class_names': ['MEL', 'NV', 'BCC', 'AK', 'BKL', 'DF', 'VASC', 'SCC', 'UNK'],\n",
        "    \n",
        "    # New progressive training parameters\n",
        "    'stage1_epochs': 20,\n",
        "    'backbone_lr_factor': 0.1,  # Backbone learns 10x slower than classifier\n",
        "    'cosine_restart_period': 10,\n",
        "    'min_lr': 1e-7,\n",
        "    \n",
        "    # Enhanced augmentation parameters\n",
        "    'tta_enabled': True,  # Test Time Augmentation\n",
        "    'mixup_alpha': 0.2,   # For future Mixup implementation\n",
        "    'cutmix_prob': 0.3,   # For future CutMix implementation\n",
        "}\n",
        "\n",
        "print(\"Enhanced configuration loaded:\")\n",
        "for key, value in CONFIG.items():\n",
        "    print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df5fc23c",
      "metadata": {
        "id": "df5fc23c"
      },
      "source": [
        "## Data Loading and Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a649f3e9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a649f3e9",
        "outputId": "a0d95365-8ae7-4b0e-89b9-70ce0129d91a"
      },
      "outputs": [],
      "source": [
        "# Initialize data loader and preprocessor\n",
        "data_loader = AzureBlobLoader(CONFIG['azure_account'])\n",
        "preprocessor = ImagePreprocessor()\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs(CONFIG['data_dir'], exist_ok=True)\n",
        "os.makedirs(CONFIG['images_dir'], exist_ok=True)\n",
        "\n",
        "# Load metadata directly from Azure (no need for local download first)\n",
        "print(\"Loading metadata from Azure...\")\n",
        "try:\n",
        "    # Load metadata using the correct metadata_type parameters\n",
        "    metadata_df = data_loader.load_metadata('isic', 'training_metadata')\n",
        "    ground_truth_df = data_loader.load_metadata('isic', 'ground_truth')\n",
        "\n",
        "    # Save locally for future use\n",
        "    metadata_path = os.path.join(CONFIG['data_dir'], CONFIG['metadata_file'])\n",
        "    ground_truth_path = os.path.join(CONFIG['data_dir'], CONFIG['ground_truth_file'])\n",
        "\n",
        "    metadata_df.to_csv(metadata_path, index=False)\n",
        "    ground_truth_df.to_csv(ground_truth_path, index=False)\n",
        "    print(\"Metadata loaded and saved locally!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading metadata from Azure: {e}\")\n",
        "    print(\"Trying to load from local files...\")\n",
        "\n",
        "    # Fallback to local files\n",
        "    metadata_path = os.path.join(CONFIG['data_dir'], CONFIG['metadata_file'])\n",
        "    ground_truth_path = os.path.join(CONFIG['data_dir'], CONFIG['ground_truth_file'])\n",
        "\n",
        "    if os.path.exists(metadata_path) and os.path.exists(ground_truth_path):\n",
        "        metadata_df = pd.read_csv(metadata_path)\n",
        "        ground_truth_df = pd.read_csv(ground_truth_path)\n",
        "        print(\"Loaded metadata from local files!\")\n",
        "    else:\n",
        "        print(\"No local metadata files found. Please check your data setup.\")\n",
        "        raise FileNotFoundError(\"Could not load metadata from Azure or local files\")\n",
        "\n",
        "# Merge metadata with ground truth\n",
        "df = pd.merge(metadata_df, ground_truth_df, on='image')\n",
        "print(f\"Loaded {len(df)} images with metadata\")\n",
        "print(\"\\nDataset columns:\", df.columns.tolist())\n",
        "print(\"\\nClass distribution:\")\n",
        "diagnosis_cols = [col for col in df.columns if col not in ['image', 'age_approx', 'anatom_site_general', 'sex']]\n",
        "\n",
        "# Fix: Ensure diagnosis columns are numeric before summing\n",
        "for col in diagnosis_cols:\n",
        "    # Convert to numeric, replacing non-numeric values with 0\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
        "    count = df[col].sum()\n",
        "    if count > 0:\n",
        "        print(f\"  {col}: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04675805",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04675805",
        "outputId": "b6dda455-2781-4d47-8c3b-6420629fdd96"
      },
      "outputs": [],
      "source": [
        "# Sample data for training (to speed up development)\n",
        "print(f\"Sampling {CONFIG['num_images']} images for training...\")\n",
        "\n",
        "# Create target column for classification\n",
        "df['target'] = df[diagnosis_cols].idxmax(axis=1)\n",
        "df['target_idx'] = LabelEncoder().fit_transform(df['target'])\n",
        "\n",
        "# Stratified sampling to maintain class distribution\n",
        "from sklearn.model_selection import train_test_split\n",
        "sampled_df, _ = train_test_split(\n",
        "    df,\n",
        "    train_size=CONFIG['num_images'],\n",
        "    stratify=df['target'],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Sampled dataset class distribution:\")\n",
        "for i, class_name in enumerate(CONFIG['class_names']):\n",
        "    count = (sampled_df['target_idx'] == i).sum()\n",
        "    print(f\"  {class_name}: {count}\")\n",
        "\n",
        "# Check which images need to be downloaded\n",
        "existing_images = set()\n",
        "if os.path.exists(CONFIG['images_dir']):\n",
        "    existing_images = set(os.listdir(CONFIG['images_dir']))\n",
        "    existing_images = {img.replace('.jpg', '') for img in existing_images if img.endswith('.jpg')}\n",
        "\n",
        "images_to_download = []\n",
        "for image_id in sampled_df['image']:\n",
        "    if image_id not in existing_images:\n",
        "        images_to_download.append(image_id)\n",
        "\n",
        "print(f\"Found {len(existing_images)} existing images\")\n",
        "print(f\"Need to download {len(images_to_download)} new images\")\n",
        "\n",
        "# Download missing images using the correct method\n",
        "if images_to_download:\n",
        "    print(\"Downloading images... This may take a while.\")\n",
        "    try:\n",
        "        successful, failed = data_loader.download_batch(\n",
        "            images_to_download,\n",
        "            dataset='isic',\n",
        "            local_dir=CONFIG['images_dir'],\n",
        "            max_workers=5\n",
        "        )\n",
        "        print(f\"Downloaded {len(successful)} images, {len(failed)} errors\")\n",
        "\n",
        "        if failed:\n",
        "            print(f\"Failed to download: {failed[:5]}...\")  # Show first 5 failures\n",
        "    except Exception as e:\n",
        "        print(f\"Error during batch download: {e}\")\n",
        "        print(\"You may need to check your network connection or Azure access.\")\n",
        "else:\n",
        "    print(\"All required images already downloaded!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5be83f2e",
      "metadata": {
        "id": "5be83f2e"
      },
      "source": [
        "## Custom Dataset and Data Transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10a86e25",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10a86e25",
        "outputId": "e98e6cfe-d34a-45ab-a5e4-2d10f9ab664c"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader # Import Dataset and DataLoader\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision.transforms as transforms # Import transforms here for fallback\n",
        "\n",
        "class SkinLesionDataset(Dataset):\n",
        "    \"\"\"Custom dataset for skin lesion images with advanced preprocessing\"\"\"\n",
        "\n",
        "    def __init__(self, dataframe, images_dir, transform=None, preprocessing=True):\n",
        "        # Create a clean copy of the dataframe and reset index\n",
        "        self.df = dataframe.copy().reset_index(drop=True)\n",
        "        self.images_dir = images_dir\n",
        "        self.transform = transform\n",
        "        self.preprocessing = preprocessing\n",
        "\n",
        "        # Ensure ImagePreprocessor is imported if not already available\n",
        "        try:\n",
        "            from image_preprocessor import ImagePreprocessor\n",
        "            self.preprocessor = ImagePreprocessor()\n",
        "        except ImportError:\n",
        "            print(\"Warning: ImagePreprocessor not found. Preprocessing will be disabled.\")\n",
        "            self.preprocessor = None\n",
        "            self.preprocessing = False\n",
        "\n",
        "        # Filter only images that exist and ensure target_idx column exists\n",
        "        self.df = self._filter_existing_images()\n",
        "        self._validate_columns() # Ensure target_idx exists\n",
        "        print(f\"Dataset initialized with {len(self.df)} valid images\")\n",
        "\n",
        "    def _validate_columns(self):\n",
        "        \"\"\"Ensure required columns exist in the dataframe\"\"\"\n",
        "        required_columns = ['image', 'target_idx']\n",
        "        for col in required_columns:\n",
        "            if col not in self.df.columns:\n",
        "                if col == 'target_idx' and 'target' in self.df.columns:\n",
        "                    # Create target_idx from target if it doesn't exist\n",
        "                    from sklearn.preprocessing import LabelEncoder\n",
        "                    le = LabelEncoder()\n",
        "                    self.df['target_idx'] = le.fit_transform(self.df['target'])\n",
        "                    print(f\"Created {col} column from 'target' column\")\n",
        "                else:\n",
        "                    raise ValueError(f\"Required column '{col}' not found in dataframe. Available columns: {list(self.df.columns)}\")\n",
        "\n",
        "\n",
        "    def _filter_existing_images(self):\n",
        "        \"\"\"Filter dataframe to only include images that exist on disk\"\"\"\n",
        "        print(f\"Filtering {len(self.df)} images for existing files...\")\n",
        "\n",
        "        # Create a boolean mask for existing images\n",
        "        existing_mask = []\n",
        "        for idx, row in self.df.iterrows():\n",
        "            image_path = os.path.join(self.images_dir, f\"{row['image']}.jpg\")\n",
        "            exists = os.path.exists(image_path)\n",
        "            existing_mask.append(exists)\n",
        "\n",
        "            # Debug: print first few missing files\n",
        "            if not exists and len([x for x in existing_mask if not x]) <= 3:\n",
        "                print(f\"Missing image: {image_path}\")\n",
        "\n",
        "\n",
        "        # Filter the dataframe using the mask - this preserves all columns\n",
        "        filtered_df = self.df[existing_mask].copy().reset_index(drop=True)\n",
        "\n",
        "        missing_count = len(self.df) - len(filtered_df)\n",
        "        if missing_count > 0:\n",
        "            print(f\"Warning: {missing_count} images not found on disk and will be skipped\")\n",
        "\n",
        "        return filtered_df\n",
        "\n",
        "    # Add the __len__ method\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the number of samples in the dataset\"\"\"\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            # Get image info\n",
        "            row = self.df.iloc[idx]\n",
        "            image_id = row['image']\n",
        "\n",
        "            # Ensure target_idx exists and is valid\n",
        "            if 'target_idx' not in row:\n",
        "                raise KeyError(f\"target_idx column not found. Available columns: {list(row.index)}\")\n",
        "\n",
        "            label = int(row['target_idx'])\n",
        "\n",
        "            # Load image\n",
        "            image_path = os.path.join(self.images_dir, f\"{image_id}.jpg\")\n",
        "\n",
        "            if not os.path.exists(image_path):\n",
        "                raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
        "\n",
        "            # Load image using PIL\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "            # Apply custom preprocessing if enabled and preprocessor is available\n",
        "            if self.preprocessing and self.preprocessor:\n",
        "                try:\n",
        "                    # Convert PIL to numpy for preprocessing\n",
        "                    image_np = np.array(image)\n",
        "\n",
        "                    # The detect_circular_vignette method returns a tuple (image_array, int, int)\n",
        "                    # FIXED: Always unpack the tuple properly\n",
        "                    processed_result = self.preprocessor.detect_circular_vignette(image_np)\n",
        "\n",
        "                    # The method always returns a tuple (image_array, width, height)\n",
        "                    if isinstance(processed_result, tuple) and len(processed_result) >= 1:\n",
        "                        processed_image = processed_result[0]  # Get the image array\n",
        "                    else:\n",
        "                        # Fallback to original image\n",
        "                        processed_image = image_np\n",
        "                        print(f\"Warning: Unexpected preprocessor output for {image_id}, using original\")\n",
        "\n",
        "                    # Ensure the processed image is valid\n",
        "                    if processed_image is None or processed_image.size == 0:\n",
        "                         processed_image = image_np\n",
        "                         print(f\"Warning: Empty processed image for {image_id}, using original\")\n",
        "\n",
        "\n",
        "                    # Convert back to PIL\n",
        "                    image = Image.fromarray(processed_image.astype(np.uint8))\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning: Preprocessing failed for {image_id}: {e}\")\n",
        "                    # Continue with original image if preprocessing fails\n",
        "\n",
        "            # Apply transforms\n",
        "            if self.transform:\n",
        "                try:\n",
        "                    image = self.transform(image)\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning: Transform failed for {image_id}: {e}\")\n",
        "                    # Create a fallback tensor\n",
        "                    image = torch.zeros(3, 224, 224)\n",
        "            else:\n",
        "                # Convert to tensor if no transform provided\n",
        "                # Use the transforms import from the top of the cell\n",
        "                image = transforms.ToTensor()(image)\n",
        "\n",
        "            # Final validation - ensure the image is a proper tensor\n",
        "            if not isinstance(image, torch.Tensor):\n",
        "                print(f\"Warning: Image for {image_id} is not a tensor after transforms\")\n",
        "                image = torch.zeros(3, 224, 224)\n",
        "\n",
        "            # Check tensor shape (assuming 3 channel, 224x224)\n",
        "            if image.shape != (3, 224, 224):\n",
        "                print(f\"Warning: Image {image_id} has wrong shape {image.shape}, creating fallback\")\n",
        "                image = torch.zeros(3, 224, 224)\n",
        "\n",
        "\n",
        "            return image, label, image_id\n",
        "\n",
        "        except FileNotFoundError:\n",
        "             # Handle file not found specifically\n",
        "             print(f\"Error: Image file not found at {image_path} for index {idx}. Returning black image.\")\n",
        "             # Return a black image and a random label as fallback for file not found\n",
        "             black_image = torch.zeros(3, 224, 224)\n",
        "             # Use a random label to avoid biasing towards class 0\n",
        "             fallback_label = np.random.randint(0, len(CONFIG['class_names'])) # Use CONFIG from global scope\n",
        "             fallback_id = f\"missing_image_{idx}\"\n",
        "             return black_image, fallback_label, fallback_id\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image at index {idx} with ID {image_id if 'image_id' in locals() else 'N/A'}: {e}\")\n",
        "            # Return a black image and a random label for other errors\n",
        "            black_image = torch.zeros(3, 224, 224)\n",
        "            # Use a random label to avoid biasing towards class 0\n",
        "            fallback_label = np.random.randint(0, len(CONFIG['class_names'])) # Use CONFIG from global scope\n",
        "            fallback_id = f\"error_processing_{idx}\"\n",
        "\n",
        "\n",
        "            return black_image, fallback_label, fallback_id\n",
        "\n",
        "print(\"SkinLesionDataset class defined with error handling and fallback!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41fc16d4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41fc16d4",
        "outputId": "5e483706-849c-49ff-b222-f978996f49cc"
      },
      "outputs": [],
      "source": [
        "# # Define data transforms optimized for medical images\n",
        "# train_transforms = transforms.Compose([\n",
        "#     transforms.Resize((CONFIG['image_size'], CONFIG['image_size'])),\n",
        "#     transforms.RandomHorizontalFlip(p=0.5),\n",
        "#     transforms.RandomVerticalFlip(p=0.5),\n",
        "#     transforms.RandomRotation(degrees=20),\n",
        "#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "#     transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
        "# ])\n",
        "\n",
        "# val_transforms = transforms.Compose([\n",
        "#     transforms.Resize((CONFIG['image_size'], CONFIG['image_size'])),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "# ])\n",
        "\n",
        "# print(\"Data transforms defined!\")\n",
        "# print(\"Training transforms include:\")\n",
        "# print(\"  - Resize to 224x224\")\n",
        "# print(\"  - Random horizontal/vertical flips\")\n",
        "# print(\"  - Random rotation (±20°)\")\n",
        "# print(\"  - Color jittering\")\n",
        "# print(\"  - Random affine transformations\")\n",
        "# print(\"  - ImageNet normalization\")\n",
        "# print(\"\\nValidation transforms include:\")\n",
        "# print(\"  - Resize to 224x224\")\n",
        "# print(\"  - ImageNet normalization only\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b92a54c7",
      "metadata": {},
      "source": [
        "### Enhanced data augmentation optimized for medical images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e25e4ad3",
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchvision.transforms import InterpolationMode\n",
        "\n",
        "# Advanced training transforms with medical-specific augmentations\n",
        "train_transforms = transforms.Compose([\n",
        "    # Resize with some padding for better random crops\n",
        "    transforms.Resize((CONFIG['image_size'] + 32, CONFIG['image_size'] + 32), \n",
        "                     interpolation=InterpolationMode.BILINEAR),\n",
        "    \n",
        "    # Random resized crop with medical-appropriate scale\n",
        "    transforms.RandomResizedCrop(CONFIG['image_size'], \n",
        "                                scale=(0.8, 1.0), \n",
        "                                ratio=(0.9, 1.1),\n",
        "                                interpolation=InterpolationMode.BILINEAR),\n",
        "    \n",
        "    # Medical image specific flips\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomVerticalFlip(p=0.5),\n",
        "    \n",
        "    # Enhanced rotation for skin lesions\n",
        "    transforms.RandomRotation(degrees=45, interpolation=InterpolationMode.BILINEAR),\n",
        "    \n",
        "    # Advanced color augmentation for medical images\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.15),\n",
        "    \n",
        "    # Affine transformations\n",
        "    transforms.RandomAffine(degrees=0, \n",
        "                           translate=(0.1, 0.1), \n",
        "                           scale=(0.85, 1.15),\n",
        "                           shear=10,\n",
        "                           interpolation=InterpolationMode.BILINEAR),\n",
        "    \n",
        "    # Advanced augmentations\n",
        "    transforms.RandomApply([\n",
        "        transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0))\n",
        "    ], p=0.3),\n",
        "    \n",
        "    transforms.RandomApply([\n",
        "        transforms.RandomAdjustSharpness(sharpness_factor=2)\n",
        "    ], p=0.3),\n",
        "    \n",
        "    # Convert to tensor\n",
        "    transforms.ToTensor(),\n",
        "    \n",
        "    # ImageNet normalization\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    \n",
        "    # Random erasing (cutout) - very effective for medical images\n",
        "    transforms.RandomErasing(p=0.25, scale=(0.02, 0.15), ratio=(0.3, 3.3), value=0)\n",
        "])\n",
        "\n",
        "# Test Time Augmentation transforms (for inference)\n",
        "tta_transforms = [\n",
        "    # Original\n",
        "    transforms.Compose([\n",
        "        transforms.Resize((CONFIG['image_size'], CONFIG['image_size'])),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    \n",
        "    # Horizontal flip\n",
        "    transforms.Compose([\n",
        "        transforms.Resize((CONFIG['image_size'], CONFIG['image_size'])),\n",
        "        transforms.RandomHorizontalFlip(p=1.0),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    \n",
        "    # Vertical flip\n",
        "    transforms.Compose([\n",
        "        transforms.Resize((CONFIG['image_size'], CONFIG['image_size'])),\n",
        "        transforms.RandomVerticalFlip(p=1.0),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    \n",
        "    # Slight rotation\n",
        "    transforms.Compose([\n",
        "        transforms.Resize((CONFIG['image_size'], CONFIG['image_size'])),\n",
        "        transforms.RandomRotation(degrees=15),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "]\n",
        "\n",
        "# Validation transforms remain simple\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize((CONFIG['image_size'], CONFIG['image_size'])),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "print(\"Enhanced data transforms defined!\")\n",
        "print(\"Training transforms include:\")\n",
        "print(\"  - Random resized crop with medical-appropriate scale\")\n",
        "print(\"  - Enhanced rotation (±45°) and affine transformations\")\n",
        "print(\"  - Advanced color jittering optimized for skin lesions\")\n",
        "print(\"  - Gaussian blur and sharpness adjustments\")\n",
        "print(\"  - Random erasing (cutout) for regularization\")\n",
        "print(\"  - Medical-specific flip augmentations\")\n",
        "print(\"\\nTest Time Augmentation (TTA) transforms available for inference\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d6d7dcc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d6d7dcc",
        "outputId": "967ab75a-3580-4c84-dbd2-f5fb999a1dc4"
      },
      "outputs": [],
      "source": [
        "# Create train/validation/test splits\n",
        "print(\"Creating train/validation/test splits...\")\n",
        "\n",
        "# First split: separate test set\n",
        "train_val_df, test_df = train_test_split(\n",
        "    sampled_df,\n",
        "    test_size=CONFIG['test_split'],\n",
        "    stratify=sampled_df['target_idx'],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Second split: separate train and validation\n",
        "train_df, val_df = train_test_split(\n",
        "    train_val_df,\n",
        "    test_size=CONFIG['validation_split']/(1-CONFIG['test_split']),\n",
        "    stratify=train_val_df['target_idx'],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Dataset splits:\")\n",
        "print(f\"  Training: {len(train_df)} images\")\n",
        "print(f\"  Validation: {len(val_df)} images\")\n",
        "print(f\"  Test: {len(test_df)} images\")\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = SkinLesionDataset(train_df, CONFIG['images_dir'], train_transforms, preprocessing=True)\n",
        "val_dataset = SkinLesionDataset(val_df, CONFIG['images_dir'], val_transforms, preprocessing=True)\n",
        "test_dataset = SkinLesionDataset(test_df, CONFIG['images_dir'], val_transforms, preprocessing=True)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=CONFIG['batch_size'],\n",
        "    shuffle=True,\n",
        "    num_workers=CONFIG['num_workers'],\n",
        "    pin_memory=True if device.type == 'cuda' else False\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=CONFIG['batch_size'],\n",
        "    shuffle=False,\n",
        "    num_workers=CONFIG['num_workers'],\n",
        "    pin_memory=True if device.type == 'cuda' else False\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=CONFIG['batch_size'],\n",
        "    shuffle=False,\n",
        "    num_workers=CONFIG['num_workers'],\n",
        "    pin_memory=True if device.type == 'cuda' else False\n",
        ")\n",
        "\n",
        "print(f\"Data loaders created!\")\n",
        "print(f\"Batch size: {CONFIG['batch_size']}\")\n",
        "print(f\"Number of workers: {CONFIG['num_workers']}\")\n",
        "print(f\"Pin memory: {device.type == 'cuda'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41778dd2",
      "metadata": {
        "id": "41778dd2"
      },
      "source": [
        "## Data Visualization and Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa101446",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "aa101446",
        "outputId": "bf2a6e79-85fc-4d06-89ee-3a89def9363d"
      },
      "outputs": [],
      "source": [
        "# Visualize class distribution\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "datasets = [('Training', train_df), ('Validation', val_df), ('Test', test_df)]\n",
        "colors = plt.cm.Set3(np.linspace(0, 1, len(CONFIG['class_names'])))\n",
        "\n",
        "for idx, (name, df) in enumerate(datasets):\n",
        "    # Ensure all possible class indices are considered, even if some have 0 counts\n",
        "    class_counts = [sum(df['target_idx'] == i) for i in range(len(CONFIG['class_names']))]\n",
        "\n",
        "    axes[idx].bar(CONFIG['class_names'], class_counts, color=colors)\n",
        "    axes[idx].set_title(f'{name} Set Class Distribution')\n",
        "    axes[idx].set_xlabel('Diagnosis')\n",
        "    axes[idx].set_ylabel('Count')\n",
        "    axes[idx].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # Add count labels on bars\n",
        "    for i, count in enumerate(class_counts):\n",
        "        # Only add label if count is greater than 0\n",
        "        if count > 0:\n",
        "            axes[idx].text(i, count + 0.5, str(count), ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate class weights for handling imbalanced data\n",
        "unique_classes_in_train = np.unique(train_df['target_idx'])\n",
        "print(f\"Classes present in training: {unique_classes_in_train}\")\n",
        "\n",
        "# Only calculate weights for classes that exist in training\n",
        "if len(unique_classes_in_train) > 1:\n",
        "    class_weights = compute_class_weight('balanced', classes=unique_classes_in_train, y=train_df['target_idx'])\n",
        "\n",
        "    # Create a full weights tensor for all 9 classes\n",
        "    full_class_weights = np.ones(len(CONFIG['class_names']))  # Start with 1.0 instead of 0\n",
        "    for class_idx, weight in zip(unique_classes_in_train, class_weights):\n",
        "        full_class_weights[class_idx] = weight\n",
        "\n",
        "    # For missing classes, use a small weight instead of 0\n",
        "    for i in range(len(CONFIG['class_names'])):\n",
        "        if i not in unique_classes_in_train:\n",
        "            full_class_weights[i] = 0.1  # Small weight for missing classes\n",
        "else:\n",
        "    # Fallback: equal weights\n",
        "    full_class_weights = np.ones(len(CONFIG['class_names']))\n",
        "\n",
        "class_weights_tensor = torch.FloatTensor(full_class_weights).to(device)\n",
        "\n",
        "print(\"Class weights for balanced training:\")\n",
        "train_class_counts = np.bincount(train_df['target_idx'], minlength=len(CONFIG['class_names']))\n",
        "for i, (name, weight) in enumerate(zip(CONFIG['class_names'], full_class_weights)):\n",
        "    print(f\"  {name}: {weight:.3f} (count: {train_class_counts[i]})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d6f7047",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5d6f7047",
        "outputId": "7c3644c6-f179-47f2-fb5e-a0eaf0cb6500"
      },
      "outputs": [],
      "source": [
        "# Visualize sample images from each class\n",
        "def denormalize(tensor):\n",
        "    \"\"\"Denormalize tensor for visualization\"\"\"\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "    return tensor * std + mean\n",
        "\n",
        "def show_sample_images():\n",
        "    \"\"\"Display sample images from each diagnostic class\"\"\"\n",
        "    fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    # Get one sample from each class\n",
        "    for class_idx in range(min(9, len(CONFIG['class_names']))):\n",
        "        class_samples = train_df[train_df['target_idx'] == class_idx]\n",
        "        if len(class_samples) > 0:\n",
        "            # Get random sample\n",
        "            sample_idx = np.random.randint(0, len(class_samples))\n",
        "            sample_row = class_samples.iloc[sample_idx]\n",
        "\n",
        "            # Load and process image\n",
        "            image_path = os.path.join(CONFIG['images_dir'], f\"{sample_row['image']}.jpg\")\n",
        "            if os.path.exists(image_path):\n",
        "                from PIL import Image\n",
        "                image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "                # Apply preprocessing and transforms\n",
        "                preprocessor = ImagePreprocessor()\n",
        "                image_np = np.array(image)\n",
        "\n",
        "                # The detect_circular_vignette method returns a tuple (image_array, int, int)\n",
        "                # Unpack the tuple and use only the image array\n",
        "                processed_image_tuple = preprocessor.detect_circular_vignette(image_np)\n",
        "                processed_image = processed_image_tuple[0] # Get the image array\n",
        "\n",
        "                image_pil = Image.fromarray(processed_image.astype(np.uint8))\n",
        "\n",
        "                # Apply validation transforms (no augmentation)\n",
        "                image_tensor = val_transforms(image_pil)\n",
        "\n",
        "                # Denormalize for display\n",
        "                display_image = denormalize(image_tensor)\n",
        "                display_image = torch.clamp(display_image, 0, 1)\n",
        "\n",
        "                # Convert to numpy and transpose for matplotlib\n",
        "                display_image = display_image.permute(1, 2, 0).numpy()\n",
        "\n",
        "                axes[class_idx].imshow(display_image)\n",
        "                axes[class_idx].set_title(f'{CONFIG[\"class_names\"][class_idx]}\\n({sample_row[\"image\"]})')\n",
        "                axes[class_idx].axis('off')\n",
        "            else:\n",
        "                axes[class_idx].text(0.5, 0.5, f'{CONFIG[\"class_names\"][class_idx]}\\nImage not found',\n",
        "                                   ha='center', va='center', transform=axes[class_idx].transAxes)\n",
        "                axes[class_idx].axis('off')\n",
        "        else:\n",
        "            axes[class_idx].text(0.5, 0.5, f'{CONFIG[\"class_names\"][class_idx]}\\nNo samples',\n",
        "                               ha='center', va='center', transform=axes[class_idx].transAxes)\n",
        "            axes[class_idx].axis('off')\n",
        "\n",
        "    plt.suptitle('Sample Images from Each Diagnostic Class (After Preprocessing)', fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "show_sample_images()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac34cd78",
      "metadata": {
        "id": "ac34cd78"
      },
      "source": [
        "## VGG16 Transfer Learning Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae326f60",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae326f60",
        "outputId": "afef54c0-6cf5-4d25-e97c-7eb7ea02e73b"
      },
      "outputs": [],
      "source": [
        "# class VGG16SkinLesionClassifier(nn.Module):\n",
        "#     \"\"\"VGG16-based transfer learning model for skin lesion classification\"\"\"\n",
        "\n",
        "#     def __init__(self, num_classes=9, pretrained=True, freeze_features=True, dropout_rate=0.5):\n",
        "#         super(VGG16SkinLesionClassifier, self).__init__()\n",
        "\n",
        "#         # Load pretrained VGG16\n",
        "#         self.vgg16 = models.vgg16(pretrained=pretrained)\n",
        "\n",
        "#         # Freeze feature extraction layers if specified\n",
        "#         if freeze_features:\n",
        "#             for param in self.vgg16.features.parameters():\n",
        "#                 param.requires_grad = False\n",
        "#             print(\"VGG16 feature layers frozen\")\n",
        "#         else:\n",
        "#             print(\"VGG16 feature layers unfrozen\")\n",
        "\n",
        "#         # Get number of features from the last layer before classifier\n",
        "#         num_features = self.vgg16.classifier[6].in_features\n",
        "\n",
        "#         # Replace the classifier with our custom one\n",
        "#         self.vgg16.classifier = nn.Sequential(\n",
        "#             nn.Linear(25088, 4096),\n",
        "#             nn.ReLU(True),\n",
        "#             nn.Dropout(dropout_rate),\n",
        "#             nn.Linear(4096, 2048),\n",
        "#             nn.ReLU(True),\n",
        "#             nn.Dropout(dropout_rate),\n",
        "#             nn.Linear(2048, 1024),\n",
        "#             nn.ReLU(True),\n",
        "#             nn.Dropout(dropout_rate),\n",
        "#             nn.Linear(1024, 512),\n",
        "#             nn.ReLU(True),\n",
        "#             nn.Dropout(dropout_rate),\n",
        "#             nn.Linear(512, num_classes)\n",
        "#         )\n",
        "\n",
        "#         # Store feature extractor (all layers except the final classifier)\n",
        "#         self.feature_extractor = nn.Sequential(*list(self.vgg16.children())[:-1])\n",
        "#         self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
        "\n",
        "#         self.num_classes = num_classes\n",
        "#         self.dropout_rate = dropout_rate\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         \"\"\"Forward pass\"\"\"\n",
        "#         return self.vgg16(x)\n",
        "\n",
        "#     def extract_features(self, x):\n",
        "#         \"\"\"Extract features from the second-to-last layer\"\"\"\n",
        "#         # Pass through feature extraction layers\n",
        "#         x = self.vgg16.features(x)\n",
        "#         x = self.avgpool(x)\n",
        "#         x = torch.flatten(x, 1)\n",
        "\n",
        "#         # Pass through classifier layers except the last one\n",
        "#         classifier_layers = list(self.vgg16.classifier.children())[:-1]\n",
        "#         for layer in classifier_layers:\n",
        "#             x = layer(x)\n",
        "\n",
        "#         return x\n",
        "\n",
        "#     def get_model_summary(self):\n",
        "#         \"\"\"Print model architecture summary\"\"\"\n",
        "#         total_params = sum(p.numel() for p in self.parameters())\n",
        "#         trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "#         print(\"VGG16 Skin Lesion Classifier Architecture:\")\n",
        "#         print(f\"  Total parameters: {total_params:,}\")\n",
        "#         print(f\"  Trainable parameters: {trainable_params:,}\")\n",
        "#         print(f\"  Frozen parameters: {total_params - trainable_params:,}\")\n",
        "#         print(f\"  Dropout rate: {self.dropout_rate}\")\n",
        "#         print(f\"  Number of classes: {self.num_classes}\")\n",
        "\n",
        "#         return total_params, trainable_params\n",
        "\n",
        "# # Create model\n",
        "# model = VGG16SkinLesionClassifier(\n",
        "#     num_classes=CONFIG['num_classes'],\n",
        "#     pretrained=True,\n",
        "#     freeze_features=True,\n",
        "#     dropout_rate=0.5\n",
        "# )\n",
        "\n",
        "# # Move model to device\n",
        "# model = model.to(device)\n",
        "\n",
        "# # Get model summary\n",
        "# total_params, trainable_params = model.get_model_summary()\n",
        "\n",
        "# print(f\"\\nModel loaded on: {device}\")\n",
        "# print(\"VGG16 Transfer Learning Model created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4a2dcfa",
      "metadata": {},
      "source": [
        "## EfficientNet-B3 Implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29e4521b",
      "metadata": {},
      "outputs": [],
      "source": [
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "class EfficientNetB3SkinLesionClassifier(nn.Module):\n",
        "    \"\"\"EfficientNet-B3 based transfer learning model for skin lesion classification\"\"\"\n",
        "    \n",
        "    def __init__(self, num_classes=9, pretrained=True, dropout_rate=0.3):\n",
        "        super(EfficientNetB3SkinLesionClassifier, self).__init__()\n",
        "        \n",
        "        # Load pretrained EfficientNet-B3\n",
        "        if pretrained:\n",
        "            self.efficientnet = EfficientNet.from_pretrained('efficientnet-b3', num_classes=num_classes)\n",
        "        else:\n",
        "            self.efficientnet = EfficientNet.from_name('efficientnet-b3', num_classes=num_classes)\n",
        "        \n",
        "        # Get the number of features from the classifier\n",
        "        num_features = self.efficientnet._fc.in_features\n",
        "        \n",
        "        # Replace the classifier with custom medical-optimized head\n",
        "        self.efficientnet._fc = nn.Sequential(\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(num_features, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "        \n",
        "        self.num_classes = num_classes\n",
        "        self.dropout_rate = dropout_rate\n",
        "        \n",
        "        # Store the feature extractor layers for feature extraction\n",
        "        self.feature_layers = nn.Sequential(*list(self.efficientnet._fc.children())[:-1])\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass\"\"\"\n",
        "        return self.efficientnet(x)\n",
        "    \n",
        "    def extract_features(self, x):\n",
        "        \"\"\"Extract features from the second-to-last layer (256-dimensional)\"\"\"\n",
        "        # Pass through the backbone\n",
        "        x = self.efficientnet.extract_features(x)\n",
        "        x = self.efficientnet._avg_pooling(x)\n",
        "        x = x.flatten(start_dim=1)\n",
        "        x = self.efficientnet._dropout(x)\n",
        "        \n",
        "        # Pass through classifier layers except the last one\n",
        "        feature_layers = list(self.efficientnet._fc.children())[:-1]\n",
        "        for layer in feature_layers:\n",
        "            x = layer(x)\n",
        "        \n",
        "        return x  # 256-dimensional features\n",
        "    \n",
        "    def get_model_summary(self):\n",
        "        \"\"\"Print model architecture summary\"\"\"\n",
        "        total_params = sum(p.numel() for p in self.parameters())\n",
        "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "        \n",
        "        print(\"EfficientNet-B3 Skin Lesion Classifier Architecture:\")\n",
        "        print(f\"  Total parameters: {total_params:,}\")\n",
        "        print(f\"  Trainable parameters: {trainable_params:,}\")\n",
        "        print(f\"  Frozen parameters: {total_params - trainable_params:,}\")\n",
        "        print(f\"  Dropout rate: {self.dropout_rate}\")\n",
        "        print(f\"  Number of classes: {self.num_classes}\")\n",
        "        \n",
        "        return total_params, trainable_params\n",
        "\n",
        "# Create EfficientNet-B3 model\n",
        "model = EfficientNetB3SkinLesionClassifier(\n",
        "    num_classes=CONFIG['num_classes'],\n",
        "    pretrained=True,\n",
        "    dropout_rate=0.3\n",
        ")\n",
        "\n",
        "# Move model to device\n",
        "model = model.to(device)\n",
        "\n",
        "# Get model summary\n",
        "total_params, trainable_params = model.get_model_summary()\n",
        "\n",
        "print(f\"\\nModel loaded on: {device}\")\n",
        "print(\"EfficientNet-B3 Transfer Learning Model created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81360a60",
      "metadata": {
        "id": "81360a60"
      },
      "source": [
        "## Training Setup and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b29ea7ce",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b29ea7ce",
        "outputId": "6e0f6618-285e-4687-f20a-c7d789d23807"
      },
      "outputs": [],
      "source": [
        "# # Loss function with class weights for imbalanced data\n",
        "# criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "\n",
        "# # Optimizer - Adam with weight decay\n",
        "# optimizer = optim.Adam(\n",
        "#     model.parameters(),\n",
        "#     lr=CONFIG['learning_rate'],\n",
        "#     weight_decay=CONFIG['weight_decay']\n",
        "# )\n",
        "\n",
        "# # Learning rate scheduler - reduce on plateau\n",
        "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "#     optimizer,\n",
        "#     mode='min',\n",
        "#     factor=0.5,\n",
        "#     patience=5,\n",
        "#     verbose=True,\n",
        "#     min_lr=1e-7\n",
        "# )\n",
        "\n",
        "# # Early stopping setup\n",
        "# class EarlyStopping:\n",
        "#     def __init__(self, patience=10, min_delta=0, restore_best_weights=True):\n",
        "#         self.patience = patience\n",
        "#         self.min_delta = min_delta\n",
        "#         self.restore_best_weights = restore_best_weights\n",
        "#         self.best_loss = None\n",
        "#         self.counter = 0\n",
        "#         self.best_weights = None\n",
        "\n",
        "#     def __call__(self, val_loss, model):\n",
        "#         if self.best_loss is None:\n",
        "#             self.best_loss = val_loss\n",
        "#             self.save_checkpoint(model)\n",
        "#         elif val_loss < self.best_loss - self.min_delta:\n",
        "#             self.best_loss = val_loss\n",
        "#             self.counter = 0\n",
        "#             self.save_checkpoint(model)\n",
        "#         else:\n",
        "#             self.counter += 1\n",
        "\n",
        "#         if self.counter >= self.patience:\n",
        "#             if self.restore_best_weights:\n",
        "#                 model.load_state_dict(self.best_weights)\n",
        "#             return True\n",
        "#         return False\n",
        "\n",
        "#     def save_checkpoint(self, model):\n",
        "#         self.best_weights = model.state_dict().copy()\n",
        "\n",
        "# early_stopping = EarlyStopping(patience=CONFIG['patience'])\n",
        "\n",
        "# print(\"Training setup complete!\")\n",
        "# print(f\"Loss function: CrossEntropyLoss with class weights\")\n",
        "# print(f\"Optimizer: Adam (lr={CONFIG['learning_rate']}, wd={CONFIG['weight_decay']})\")\n",
        "# print(f\"Scheduler: ReduceLROnPlateau (factor=0.5, patience=5)\")\n",
        "# print(f\"Early stopping: Patience={CONFIG['patience']} epochs\")\n",
        "# print(f\"Max epochs: {CONFIG['num_epochs']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b95ff728",
      "metadata": {},
      "source": [
        "### Progressive Training Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "807ee0e0",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_progressive_optimizer_and_scheduler(model, stage='frozen'):\n",
        "    \"\"\"Create optimizer and scheduler for different training stages\"\"\"\n",
        "    \n",
        "    if stage == 'frozen':\n",
        "        # Stage 1: Only train classifier head\n",
        "        params_to_train = []\n",
        "        for name, param in model.named_parameters():\n",
        "            if '_fc' in name:  # EfficientNet classifier\n",
        "                param.requires_grad = True\n",
        "                params_to_train.append(param)\n",
        "            else:\n",
        "                param.requires_grad = False\n",
        "        \n",
        "        optimizer = optim.AdamW(\n",
        "            params_to_train,\n",
        "            lr=CONFIG['learning_rate'],\n",
        "            weight_decay=CONFIG['weight_decay'],\n",
        "            betas=(0.9, 0.999)\n",
        "        )\n",
        "        \n",
        "        print(f\"Stage 1 (Frozen): Training {sum(p.numel() for p in params_to_train):,} parameters\")\n",
        "        \n",
        "    else:  # stage == 'fine_tune'\n",
        "        # Stage 2: Fine-tune entire model with lower learning rate\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = True\n",
        "        \n",
        "        # Different learning rates for different parts\n",
        "        backbone_params = []\n",
        "        classifier_params = []\n",
        "        \n",
        "        for name, param in model.named_parameters():\n",
        "            if '_fc' in name:\n",
        "                classifier_params.append(param)\n",
        "            else:\n",
        "                backbone_params.append(param)\n",
        "        \n",
        "        optimizer = optim.AdamW([\n",
        "            {'params': backbone_params, 'lr': CONFIG['learning_rate'] * 0.1},\n",
        "            {'params': classifier_params, 'lr': CONFIG['learning_rate']}\n",
        "        ], weight_decay=CONFIG['weight_decay'], betas=(0.9, 0.999))\n",
        "        \n",
        "        print(f\"Stage 2 (Fine-tune): Training all {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
        "    \n",
        "    # Cosine annealing scheduler with warm restarts\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "        optimizer,\n",
        "        T_0=10,  # Restart every 10 epochs\n",
        "        T_mult=2,  # Double restart period each time\n",
        "        eta_min=1e-7,\n",
        "        verbose=True\n",
        "    )\n",
        "    \n",
        "    return optimizer, scheduler\n",
        "\n",
        "# Progressive training function\n",
        "def train_model_progressive():\n",
        "    \"\"\"Progressive training: frozen features → fine-tuning\"\"\"\n",
        "    print(\"Starting Progressive EfficientNet-B3 Training\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    all_train_losses = []\n",
        "    all_val_losses = []\n",
        "    all_train_accs = []\n",
        "    all_val_accs = []\n",
        "    \n",
        "    best_val_acc = 0.0\n",
        "    total_start_time = datetime.now()\n",
        "    \n",
        "    # Stage 1: Frozen backbone training\n",
        "    print(\"\\nSTAGE 1: Training with frozen backbone (20 epochs)\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    optimizer, scheduler = create_progressive_optimizer_and_scheduler(model, 'frozen')\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "    early_stopping = EarlyStopping(patience=15)\n",
        "    \n",
        "    stage1_epochs = 20\n",
        "    for epoch in range(stage1_epochs):\n",
        "        epoch_start = datetime.now()\n",
        "        print(f\"\\nStage 1 - Epoch {epoch+1}/{stage1_epochs}\")\n",
        "        \n",
        "        # Training\n",
        "        train_loss, train_acc, train_preds, train_targets = train_epoch(\n",
        "            model, train_loader, criterion, optimizer, device, epoch+1\n",
        "        )\n",
        "        \n",
        "        # Validation\n",
        "        val_loss, val_acc, val_preds, val_targets, val_probs = validate_epoch(\n",
        "            model, val_loader, criterion, device\n",
        "        )\n",
        "        \n",
        "        # Update scheduler\n",
        "        scheduler.step(epoch + val_loss / len(val_loader))\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        \n",
        "        # Store history\n",
        "        all_train_losses.append(train_loss)\n",
        "        all_val_losses.append(val_loss)\n",
        "        all_train_accs.append(train_acc)\n",
        "        all_val_accs.append(val_acc)\n",
        "        \n",
        "        # Print summary\n",
        "        epoch_time = datetime.now() - epoch_start\n",
        "        print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
        "        print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.2f}%\")\n",
        "        print(f\"  LR: {current_lr:.2e} | Time: {epoch_time.total_seconds():.1f}s\")\n",
        "        \n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            print(f\"New best validation accuracy: {best_val_acc:.2f}%\")\n",
        "        \n",
        "        if early_stopping(val_loss, model):\n",
        "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
        "            break\n",
        "    \n",
        "    # Stage 2: Fine-tuning entire model\n",
        "    print(f\"\\nSTAGE 2: Fine-tuning entire model ({CONFIG['num_epochs'] - stage1_epochs} epochs)\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    optimizer, scheduler = create_progressive_optimizer_and_scheduler(model, 'fine_tune')\n",
        "    early_stopping = EarlyStopping(patience=CONFIG['patience'])\n",
        "    \n",
        "    remaining_epochs = CONFIG['num_epochs'] - stage1_epochs\n",
        "    for epoch in range(remaining_epochs):\n",
        "        epoch_start = datetime.now()\n",
        "        print(f\"\\nStage 2 - Epoch {epoch+1}/{remaining_epochs}\")\n",
        "        \n",
        "        # Training\n",
        "        train_loss, train_acc, train_preds, train_targets = train_epoch(\n",
        "            model, train_loader, criterion, optimizer, device, stage1_epochs + epoch + 1\n",
        "        )\n",
        "        \n",
        "        # Validation\n",
        "        val_loss, val_acc, val_preds, val_targets, val_probs = validate_epoch(\n",
        "            model, val_loader, criterion, device\n",
        "        )\n",
        "        \n",
        "        # Update scheduler\n",
        "        scheduler.step(stage1_epochs + epoch + val_loss / len(val_loader))\n",
        "        \n",
        "        # Different learning rates for different parameter groups\n",
        "        backbone_lr = optimizer.param_groups[0]['lr']\n",
        "        classifier_lr = optimizer.param_groups[1]['lr']\n",
        "        \n",
        "        # Store history\n",
        "        all_train_losses.append(train_loss)\n",
        "        all_val_losses.append(val_loss)\n",
        "        all_train_accs.append(train_acc)\n",
        "        all_val_accs.append(val_acc)\n",
        "        \n",
        "        # Print summary\n",
        "        epoch_time = datetime.now() - epoch_start\n",
        "        print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
        "        print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.2f}%\")\n",
        "        print(f\"  Backbone LR: {backbone_lr:.2e} | Classifier LR: {classifier_lr:.2e}\")\n",
        "        print(f\"  Time: {epoch_time.total_seconds():.1f}s\")\n",
        "        \n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            print(f\"New best validation accuracy: {best_val_acc:.2f}%\")\n",
        "        \n",
        "        if early_stopping(val_loss, model):\n",
        "            print(f\"Early stopping triggered at epoch {stage1_epochs + epoch + 1}\")\n",
        "            break\n",
        "        \n",
        "        # Detailed metrics every 10 epochs\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"\\nDetailed Validation Metrics (Stage 2, Epoch {epoch+1}):\")\n",
        "            calculate_detailed_metrics(val_preds, val_targets, CONFIG['class_names'])\n",
        "    \n",
        "    # Training complete\n",
        "    total_time = datetime.now() - total_start_time\n",
        "    print(f\"\\n🎉 Progressive Training Complete!\")\n",
        "    print(f\"Total time: {total_time}\")\n",
        "    print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
        "    \n",
        "    # Plot training history\n",
        "    plot_training_history(all_train_losses, all_val_losses, all_train_accs, all_val_accs)\n",
        "    \n",
        "    return {\n",
        "        'train_losses': all_train_losses,\n",
        "        'val_losses': all_val_losses,\n",
        "        'train_accs': all_train_accs,\n",
        "        'val_accs': all_val_accs,\n",
        "        'best_val_acc': best_val_acc,\n",
        "        'total_time': total_time,\n",
        "        'stage1_epochs': stage1_epochs\n",
        "    }\n",
        "\n",
        "print(\"Progressive training function defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d96feaf5",
      "metadata": {
        "id": "d96feaf5"
      },
      "source": [
        "## Training Loop Implementation\n",
        "\n",
        "The following training loop includes:\n",
        "- **Comprehensive metrics tracking**: Accuracy, precision, recall, F1-score per class\n",
        "- **GPU optimization**: Efficient batch processing with mixed precision support\n",
        "- **Medical image specific validation**: Class-wise performance monitoring\n",
        "- **Robust checkpointing**: Save best models and training state\n",
        "- **Advanced visualization**: Real-time training progress and confusion matrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7b3cce0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7b3cce0",
        "outputId": "38ece0a5-27b1-4257-b7fb-a5adf1bb7312"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, train_loader, criterion, optimizer, device, epoch):\n",
        "    \"\"\"Train for one epoch with detailed metrics and debugging\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "    error_count = 0\n",
        "\n",
        "    # Progress tracking\n",
        "    batch_losses = []\n",
        "\n",
        "    for batch_idx, (images, targets, image_ids) in enumerate(train_loader):\n",
        "        # Debug: Check for problematic batches\n",
        "        if torch.any(torch.isnan(images)) or torch.any(torch.isinf(images)):\n",
        "            print(f\"Warning: NaN/Inf detected in batch {batch_idx}\")\n",
        "            continue\n",
        "\n",
        "        images, targets = images.to(device), targets.to(device)\n",
        "\n",
        "        # Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        try:\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "\n",
        "            # Check for NaN outputs\n",
        "            if torch.any(torch.isnan(outputs)):\n",
        "                print(f\"Warning: NaN outputs in batch {batch_idx}\")\n",
        "                continue\n",
        "\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Check for NaN loss\n",
        "            if torch.isnan(loss):\n",
        "                print(f\"Warning: NaN loss in batch {batch_idx}\")\n",
        "                continue\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping to prevent exploding gradients\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            # Statistics\n",
        "            running_loss += loss.item()\n",
        "            batch_losses.append(loss.item())\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "\n",
        "            # Store for detailed metrics\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "            # Print progress every 20 batches with more details\n",
        "            if batch_idx % 20 == 0 and batch_idx > 0:\n",
        "                avg_loss = running_loss / (batch_idx + 1)\n",
        "                accuracy = 100. * correct / total\n",
        "                print(f'  Batch {batch_idx}/{len(train_loader)}: Loss: {avg_loss:.4f}, Acc: {accuracy:.2f}%, Errors: {error_count}')\n",
        "\n",
        "                # Debug: Show unique predictions and targets in this batch\n",
        "                unique_preds = np.unique(predicted.cpu().numpy())\n",
        "                unique_targets = np.unique(targets.cpu().numpy())\n",
        "                print(f'    Batch predictions: {unique_preds}, targets: {unique_targets}')\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in batch {batch_idx}: {e}\")\n",
        "            error_count += 1\n",
        "            continue\n",
        "\n",
        "    # Calculate final metrics\n",
        "    if len(batch_losses) > 0:\n",
        "        epoch_loss = running_loss / len(batch_losses)\n",
        "    else:\n",
        "        epoch_loss = float('inf')\n",
        "\n",
        "    epoch_acc = 100. * correct / total if total > 0 else 0.0\n",
        "\n",
        "    print(f\"  Epoch complete: {error_count} batch errors out of {len(train_loader)} batches\")\n",
        "    return epoch_loss, epoch_acc, all_predictions, all_targets\n",
        "\n",
        "def validate_epoch(model, val_loader, criterion, device):\n",
        "    \"\"\"Validate for one epoch with detailed metrics\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "    all_probabilities = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets, image_ids in val_loader:\n",
        "            images, targets = images.to(device), targets.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Get probabilities\n",
        "            probabilities = F.softmax(outputs, dim=1)\n",
        "\n",
        "            # Statistics\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "\n",
        "            # Store for detailed metrics\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "            all_probabilities.extend(probabilities.cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    epoch_loss = running_loss / len(val_loader)\n",
        "    epoch_acc = 100. * correct / total\n",
        "\n",
        "    return epoch_loss, epoch_acc, all_predictions, all_targets, all_probabilities\n",
        "\n",
        "print(\"Training and validation functions defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aae79e0b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aae79e0b",
        "outputId": "bc994972-ae5b-407e-f81f-c93aa4b23c3f"
      },
      "outputs": [],
      "source": [
        "def calculate_detailed_metrics(predictions, targets, class_names):\n",
        "    \"\"\"Calculate and display detailed classification metrics\"\"\"\n",
        "    from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "    # Classification report\n",
        "    # Fix: Explicitly specify labels to handle cases where some classes are missing in the subset\n",
        "    report = classification_report(\n",
        "        targets,\n",
        "        predictions,\n",
        "        target_names=class_names,\n",
        "        labels=np.arange(len(class_names)), # Specify all possible class indices\n",
        "        output_dict=True,\n",
        "        zero_division=0 # Handle cases with zero division (e.g., no samples for a class)\n",
        "    )\n",
        "\n",
        "    # Print formatted report\n",
        "    print(\"\\nDetailed Classification Metrics:\")\n",
        "    print(\"=\" * 60)\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        # Check if the class is in the report (it should be with labels specified)\n",
        "        if str(i) in report: # classification_report uses string keys for labels\n",
        "            metrics = report[str(i)]\n",
        "            # Handle cases where a class might have 0 support\n",
        "            precision = metrics.get('precision', 0.0)\n",
        "            recall = metrics.get('recall', 0.0)\n",
        "            f1_score = metrics.get('f1-score', 0.0)\n",
        "            support = metrics.get('support', 0)\n",
        "\n",
        "            print(f\"{class_name:>8}: P={precision:.3f}, R={recall:.3f}, F1={f1_score:.3f}, Support={int(support)}\")\n",
        "        else:\n",
        "             # This case should ideally not happen after specifying labels, but as a fallback\n",
        "             print(f\"{class_name:>8}: P=N/A, R=N/A, F1=N/A, Support=0\")\n",
        "\n",
        "\n",
        "    print(\"-\" * 60)\n",
        "    # Ensure global metrics are accessed correctly\n",
        "    print(f\"{'Accuracy':>8}: {report['accuracy']:.3f}\")\n",
        "    print(f\"{'Macro Avg':>8}: P={report['macro avg']['precision']:.3f}, R={report['macro avg']['recall']:.3f}, F1={report['macro avg']['f1-score']:.3f}\")\n",
        "    print(f\"{'Weighted Avg':>8}: P={report['weighted avg']['precision']:.3f}, R={report['weighted avg']['recall']:.3f}, F1={report['weighted avg']['f1-score']:.3f}\")\n",
        "\n",
        "    return report\n",
        "\n",
        "def plot_confusion_matrix(predictions, targets, class_names, title=\"Confusion Matrix\"):\n",
        "    \"\"\"Plot confusion matrix with medical focus\"\"\"\n",
        "    # Ensure confusion matrix is generated for all classes\n",
        "    cm = confusion_matrix(targets, predictions, labels=np.arange(len(class_names)))\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title(f'{title}\\n(Medical Image Classification)', fontsize=14)\n",
        "    plt.xlabel('Predicted Diagnosis', fontsize=12)\n",
        "    plt.ylabel('True Diagnosis', fontsize=12)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.yticks(rotation=0)\n",
        "\n",
        "    # Add accuracy for each class\n",
        "    # Avoid division by zero if a class has no true samples\n",
        "    class_sums = cm.sum(axis=1)\n",
        "    class_accuracies = np.divide(cm.diagonal(), class_sums, out=np.zeros_like(cm.diagonal(), dtype=float), where=class_sums!=0)\n",
        "\n",
        "    for i, acc in enumerate(class_accuracies):\n",
        "        # Only display accuracy if the class exists in the true labels\n",
        "        if class_sums[i] > 0:\n",
        "             plt.text(len(class_names) + 0.1, i + 0.5, f'Acc: {acc:.2f}',\n",
        "                    va='center', fontweight='bold')\n",
        "        else:\n",
        "             plt.text(len(class_names) + 0.1, i + 0.5, f'Acc: N/A',\n",
        "                    va='center', fontweight='bold', color='gray')\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return cm\n",
        "\n",
        "def plot_training_history(train_losses, val_losses, train_accs, val_accs):\n",
        "    \"\"\"Plot training history with medical context\"\"\"\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Loss plot\n",
        "    ax1.plot(epochs, train_losses, 'b-', label='Training Loss', linewidth=2)\n",
        "    ax1.plot(epochs, val_losses, 'r-', label='Validation Loss', linewidth=2)\n",
        "    ax1.set_title('Model Loss (Medical Image Classification)', fontsize=14)\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Accuracy plot\n",
        "    ax2.plot(epochs, train_accs, 'b-', label='Training Accuracy', linewidth=2)\n",
        "    ax2.plot(epochs, val_accs, 'r-', label='Validation Accuracy', linewidth=2)\n",
        "    ax2.set_title('Model Accuracy (Skin Lesion Diagnosis)', fontsize=14)\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Accuracy (%)')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"Metrics and visualization functions defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb064b4e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb064b4e",
        "outputId": "63275fae-e3da-4952-fa86-e08cb4070ddf"
      },
      "outputs": [],
      "source": [
        "# # Main training loop with comprehensive monitoring\n",
        "# def train_model():\n",
        "#     \"\"\"Complete training loop with medical image focus\"\"\"\n",
        "#     print(\"Starting VGG16 Transfer Learning Training for Skin Lesion Classification\")\n",
        "#     print(\"=\" * 80)\n",
        "\n",
        "#     # Training history\n",
        "#     train_losses = []\n",
        "#     val_losses = []\n",
        "#     train_accs = []\n",
        "#     val_accs = []\n",
        "\n",
        "#     best_val_acc = 0.0\n",
        "#     start_time = datetime.now()\n",
        "\n",
        "#     for epoch in range(CONFIG['num_epochs']):\n",
        "#         epoch_start = datetime.now()\n",
        "#         print(f\"\\nEpoch {epoch+1}/{CONFIG['num_epochs']}\")\n",
        "#         print(\"-\" * 50)\n",
        "\n",
        "#         # Training phase\n",
        "#         print(\"Training phase...\")\n",
        "#         train_loss, train_acc, train_preds, train_targets = train_epoch(\n",
        "#             model, train_loader, criterion, optimizer, device, epoch+1\n",
        "#         )\n",
        "\n",
        "#         # Validation phase\n",
        "#         print(\"Validation phase...\")\n",
        "#         val_loss, val_acc, val_preds, val_targets, val_probs = validate_epoch(\n",
        "#             model, val_loader, criterion, device\n",
        "#         )\n",
        "\n",
        "#         # Update learning rate\n",
        "#         scheduler.step(val_loss)\n",
        "#         current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "#         # Store history\n",
        "#         train_losses.append(train_loss)\n",
        "#         val_losses.append(val_loss)\n",
        "#         train_accs.append(train_acc)\n",
        "#         val_accs.append(val_acc)\n",
        "\n",
        "#         # Calculate epoch time\n",
        "#         epoch_time = datetime.now() - epoch_start\n",
        "\n",
        "#         # Print epoch summary\n",
        "#         print(f\"\\nEpoch {epoch+1} Summary:\")\n",
        "#         print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
        "#         print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.2f}%\")\n",
        "#         print(f\"  Learning Rate: {current_lr:.2e}\")\n",
        "#         print(f\"  Time: {epoch_time.total_seconds():.1f}s\")\n",
        "\n",
        "#         # Check if best model\n",
        "#         if val_acc > best_val_acc:\n",
        "#             best_val_acc = val_acc\n",
        "#             print(f\"  New best validation accuracy: {best_val_acc:.2f}%\")\n",
        "\n",
        "#             # Save best model (we'll implement this next)\n",
        "#             best_model_state = model.state_dict().copy()\n",
        "\n",
        "#         # Early stopping check\n",
        "#         if early_stopping(val_loss, model):\n",
        "#             print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
        "#             break\n",
        "\n",
        "#         # Show detailed metrics every 5 epochs\n",
        "#         if (epoch + 1) % 5 == 0:\n",
        "#             print(f\"\\nDetailed Validation Metrics (Epoch {epoch+1}):\")\n",
        "#             calculate_detailed_metrics(val_preds, val_targets, CONFIG['class_names'])\n",
        "\n",
        "#     # Training complete\n",
        "#     total_time = datetime.now() - start_time\n",
        "#     print(f\"\\nTraining Complete!\")\n",
        "#     print(f\"Total time: {total_time}\")\n",
        "#     print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
        "\n",
        "#     # Plot training history\n",
        "#     plot_training_history(train_losses, val_losses, train_accs, val_accs)\n",
        "\n",
        "#     return {\n",
        "#         'train_losses': train_losses,\n",
        "#         'val_losses': val_losses,\n",
        "#         'train_accs': train_accs,\n",
        "#         'val_accs': val_accs,\n",
        "#         'best_val_acc': best_val_acc,\n",
        "#         'total_time': total_time\n",
        "#     }\n",
        "\n",
        "# print(\"Main training function defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eL7NS6azLv--",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eL7NS6azLv--",
        "outputId": "13334043-b1a9-482d-909b-2e2d2da52c5c"
      },
      "outputs": [],
      "source": [
        "def validate_dataset(dataset, num_samples=10):\n",
        "    \"\"\"Validate that the dataset is working properly\"\"\"\n",
        "    print(\"Validating dataset...\")\n",
        "\n",
        "    for i in range(min(num_samples, len(dataset))):\n",
        "        try:\n",
        "            image, label, image_id = dataset[i]\n",
        "            print(f\"Sample {i}: Image shape: {image.shape}, Label: {label}, ID: {image_id}\")\n",
        "\n",
        "            # Check for problematic images\n",
        "            if torch.any(torch.isnan(image)):\n",
        "                print(f\"  WARNING: NaN values in image {image_id}\")\n",
        "            if torch.any(torch.isinf(image)):\n",
        "                print(f\"  WARNING: Inf values in image {image_id}\")\n",
        "            if torch.all(image == 0):\n",
        "                print(f\"  WARNING: All-zero image {image_id}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ERROR loading sample {i}: {e}\")\n",
        "\n",
        "print(\"Dataset validation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e0861b5",
      "metadata": {},
      "source": [
        "### Execute Progressive Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fae77f6",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"ISIC 2019 Skin Lesion Classification - Enhanced Training\")\n",
        "print(\"Model: EfficientNet-B3 with Progressive Training\")\n",
        "print(f\"Dataset: {len(train_dataset)} train, {len(val_dataset)} val, {len(test_dataset)} test\")\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Image size: {CONFIG['image_size']}x{CONFIG['image_size']}\")\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "# Validate datasets\n",
        "validate_dataset(train_dataset, num_samples=5)\n",
        "\n",
        "# Execute progressive training\n",
        "training_history = train_model_progressive()\n",
        "\n",
        "# Save the best model\n",
        "print(\"\\nSaving best model...\")\n",
        "best_checkpoint_path = save_model_checkpoint(\n",
        "    model, None, None,  # Optimizer/scheduler not needed for final save\n",
        "    epoch=len(training_history['train_losses']),\n",
        "    val_acc=training_history['best_val_acc'],\n",
        "    val_loss=min(training_history['val_losses']),\n",
        "    config=CONFIG,\n",
        "    filename=\"efficientnet_b3_progressive_best_model.pth\"\n",
        ")\n",
        "\n",
        "print(f\"\\nProgressive Training Completed!\")\n",
        "print(f\"Best validation accuracy: {training_history['best_val_acc']:.2f}%\")\n",
        "print(f\"Total training time: {training_history['total_time']}\")\n",
        "print(f\"Stage 1 epochs: {training_history['stage1_epochs']}\")\n",
        "print(f\"Total epochs: {len(training_history['train_losses'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9994ae4",
      "metadata": {
        "id": "d9994ae4"
      },
      "source": [
        "## Model Evaluation and Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8dcbbe4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8dcbbe4",
        "outputId": "d995d4d0-3073-4399-abd9-3be006a97bf9"
      },
      "outputs": [],
      "source": [
        "def evaluate_model_comprehensive(model, test_loader, device, class_names):\n",
        "    \"\"\"Comprehensive model evaluation with medical focus\"\"\"\n",
        "    print(\"Comprehensive Model Evaluation\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "    all_probabilities = []\n",
        "    all_image_ids = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets, image_ids in test_loader:\n",
        "            images, targets = images.to(device), targets.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            probabilities = F.softmax(outputs, dim=1)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            # Store results\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "            all_probabilities.extend(probabilities.cpu().numpy())\n",
        "            all_image_ids.extend(image_ids)\n",
        "\n",
        "    # Calculate overall accuracy\n",
        "    accuracy = accuracy_score(all_targets, all_predictions)\n",
        "    print(f\"Overall Test Accuracy: {accuracy*100:.2f}%\")\n",
        "\n",
        "    # Detailed metrics\n",
        "    print(\"\\nDetailed Classification Report:\")\n",
        "    report = calculate_detailed_metrics(all_predictions, all_targets, class_names)\n",
        "\n",
        "    # Confusion matrix\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    cm = plot_confusion_matrix(all_predictions, all_targets, class_names, \"Test Set Confusion Matrix\")\n",
        "\n",
        "    # Per-class analysis\n",
        "    print(\"\\nPer-Class Performance Analysis:\")\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        class_mask = np.array(all_targets) == i\n",
        "        if np.sum(class_mask) > 0:\n",
        "            class_predictions = np.array(all_predictions)[class_mask]\n",
        "            class_accuracy = accuracy_score(np.array(all_targets)[class_mask], class_predictions)\n",
        "            print(f\"  {class_name}: {class_accuracy*100:.1f}% accuracy ({np.sum(class_mask)} samples)\")\n",
        "\n",
        "    # Most confident predictions\n",
        "    confidence_scores = np.max(all_probabilities, axis=1)\n",
        "    correct_predictions = np.array(all_predictions) == np.array(all_targets)\n",
        "\n",
        "    print(f\"\\nConfidence Analysis:\")\n",
        "    print(f\"  Average confidence: {np.mean(confidence_scores):.3f}\")\n",
        "    print(f\"  Confidence on correct predictions: {np.mean(confidence_scores[correct_predictions]):.3f}\")\n",
        "    print(f\"  Confidence on incorrect predictions: {np.mean(confidence_scores[~correct_predictions]):.3f}\")\n",
        "\n",
        "    return {\n",
        "        'predictions': all_predictions,\n",
        "        'targets': all_targets,\n",
        "        'probabilities': all_probabilities,\n",
        "        'image_ids': all_image_ids,\n",
        "        'accuracy': accuracy,\n",
        "        'report': report,\n",
        "        'confusion_matrix': cm\n",
        "    }\n",
        "\n",
        "def analyze_misclassifications(model, test_loader, device, class_names, num_samples=9):\n",
        "    \"\"\"Analyze and visualize misclassified samples\"\"\"\n",
        "    print(\"Misclassification Analysis\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    model.eval()\n",
        "    misclassified_samples = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets, image_ids in test_loader:\n",
        "            images, targets = images.to(device), targets.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            probabilities = F.softmax(outputs, dim=1)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            # Find misclassified samples\n",
        "            incorrect_mask = predicted != targets\n",
        "            if incorrect_mask.any():\n",
        "                for i in range(len(images)):\n",
        "                    if incorrect_mask[i]:\n",
        "                        misclassified_samples.append({\n",
        "                            'image': images[i].cpu(),\n",
        "                            'true_label': targets[i].item(),\n",
        "                            'pred_label': predicted[i].item(),\n",
        "                            'probabilities': probabilities[i].cpu().numpy(),\n",
        "                            'image_id': image_ids[i]\n",
        "                        })\n",
        "\n",
        "            if len(misclassified_samples) >= num_samples:\n",
        "                break\n",
        "\n",
        "    # Visualize misclassifications\n",
        "    if misclassified_samples:\n",
        "        fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
        "        axes = axes.flatten()\n",
        "\n",
        "        for i, sample in enumerate(misclassified_samples[:num_samples]):\n",
        "            # Denormalize image\n",
        "            img = denormalize(sample['image'])\n",
        "            img = torch.clamp(img, 0, 1)\n",
        "            img = img.permute(1, 2, 0).numpy()\n",
        "\n",
        "            # Plot\n",
        "            axes[i].imshow(img)\n",
        "            true_class = class_names[sample['true_label']]\n",
        "            pred_class = class_names[sample['pred_label']]\n",
        "            confidence = sample['probabilities'][sample['pred_label']]\n",
        "\n",
        "            axes[i].set_title(f'True: {true_class}\\nPred: {pred_class} ({confidence:.2f})\\nID: {sample[\"image_id\"]}')\n",
        "            axes[i].axis('off')\n",
        "\n",
        "        plt.suptitle('Misclassified Samples Analysis', fontsize=16)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        print(f\"Analyzed {len(misclassified_samples[:num_samples])} misclassified samples\")\n",
        "    else:\n",
        "        print(\"No misclassifications found in the sample!\")\n",
        "\n",
        "print(\"Evaluation functions defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf7313c5",
      "metadata": {
        "id": "bf7313c5"
      },
      "source": [
        "## Model Saving and Loading Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a24e028d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a24e028d",
        "outputId": "cf1a4ef6-2211-4c78-b8f2-f825e5494304"
      },
      "outputs": [],
      "source": [
        "def save_model_checkpoint(model, optimizer, scheduler, epoch, val_acc, val_loss, config, filename=None):\n",
        "    \"\"\"Save complete model checkpoint with metadata\"\"\"\n",
        "    if filename is None:\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        filename = f\"vgg16_skinlesion_epoch_{epoch}_acc_{val_acc:.2f}_{timestamp}.pth\"\n",
        "\n",
        "    # Create models directory\n",
        "    models_dir = \"../models\"\n",
        "    os.makedirs(models_dir, exist_ok=True)\n",
        "    filepath = os.path.join(models_dir, filename)\n",
        "\n",
        "    # Prepare checkpoint data\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'val_accuracy': val_acc,\n",
        "        'val_loss': val_loss,\n",
        "        'config': config,\n",
        "        'class_names': CONFIG['class_names'],\n",
        "        'model_architecture': 'VGG16SkinLesionClassifier',\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'device': str(device)\n",
        "    }\n",
        "\n",
        "    # Save checkpoint\n",
        "    torch.save(checkpoint, filepath)\n",
        "    print(f\"Model checkpoint saved: {filename}\")\n",
        "    print(f\"   Epoch: {epoch}, Val Acc: {val_acc:.2f}%, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    return filepath\n",
        "\n",
        "def load_model_checkpoint(filepath, model, optimizer=None, scheduler=None):\n",
        "    \"\"\"Load model checkpoint with full state restoration\"\"\"\n",
        "    print(f\"Loading model checkpoint: {filepath}\")\n",
        "\n",
        "    # Load checkpoint\n",
        "    checkpoint = torch.load(filepath, map_location=device)\n",
        "\n",
        "    # Restore model state\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # Restore optimizer and scheduler if provided\n",
        "    if optimizer and 'optimizer_state_dict' in checkpoint:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    if scheduler and 'scheduler_state_dict' in checkpoint:\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "\n",
        "    # Print checkpoint info\n",
        "    print(f\"Checkpoint loaded successfully!\")\n",
        "    print(f\"   Epoch: {checkpoint['epoch']}\")\n",
        "    print(f\"   Validation Accuracy: {checkpoint['val_accuracy']:.2f}%\")\n",
        "    print(f\"   Validation Loss: {checkpoint['val_loss']:.4f}\")\n",
        "    print(f\"   Saved on: {checkpoint['timestamp']}\")\n",
        "\n",
        "    return checkpoint\n",
        "\n",
        "def save_training_results(training_history, evaluation_results, filename=None):\n",
        "    \"\"\"Save complete training and evaluation results\"\"\"\n",
        "    if filename is None:\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        filename = f\"training_results_{timestamp}.pkl\"\n",
        "\n",
        "    results_dir = \"../results\"\n",
        "    os.makedirs(results_dir, exist_ok=True)\n",
        "    filepath = os.path.join(results_dir, filename)\n",
        "\n",
        "    # Combine all results\n",
        "    complete_results = {\n",
        "        'training_history': training_history,\n",
        "        'evaluation_results': evaluation_results,\n",
        "        'config': CONFIG,\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'model_type': 'VGG16SkinLesionClassifier'\n",
        "    }\n",
        "\n",
        "    # Save results\n",
        "    with open(filepath, 'wb') as f:\n",
        "        pickle.dump(complete_results, f)\n",
        "\n",
        "    print(f\"Training results saved: {filename}\")\n",
        "    return filepath\n",
        "\n",
        "def export_model_for_inference(model, filepath=None):\n",
        "    \"\"\"Export optimized model for inference\"\"\"\n",
        "    if filepath is None:\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        filepath = f\"../models/vgg16_skinlesion_inference_{timestamp}.pth\"\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Create inference package\n",
        "    inference_package = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'class_names': CONFIG['class_names'],\n",
        "        'num_classes': CONFIG['num_classes'],\n",
        "        'image_size': CONFIG['image_size'],\n",
        "        'transforms': {\n",
        "            'mean': [0.485, 0.456, 0.406],\n",
        "            'std': [0.229, 0.224, 0.225]\n",
        "        },\n",
        "        'model_type': 'VGG16SkinLesionClassifier',\n",
        "        'export_timestamp': datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "    torch.save(inference_package, filepath)\n",
        "    print(f\"Inference model exported: {filepath}\")\n",
        "    return filepath\n",
        "\n",
        "print(\"Model saving and loading utilities defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cce00386",
      "metadata": {
        "id": "cce00386"
      },
      "source": [
        "## Execute Training and Evaluation\n",
        "\n",
        "Ready to train your VGG16 model! The cells below will:\n",
        "\n",
        "1. **Start the training process** with comprehensive monitoring\n",
        "2. **Evaluate the trained model** on the test set\n",
        "3. **Save the best model** and training results\n",
        "4. **Analyze performance** with detailed medical image metrics\n",
        "\n",
        "**Note**: Training may take 30-60 minutes depending on your GPU. The notebook includes early stopping to prevent overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4bcebc8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d4bcebc8",
        "outputId": "da3040c0-aefb-4436-83ca-fbde10cb1c1b"
      },
      "outputs": [],
      "source": [
        "# Start training process\n",
        "print(\"ISIC 2019 Skin Lesion Classification Training\")\n",
        "print(\"Model: VGG16 Transfer Learning\")\n",
        "print(f\"Dataset: {len(train_dataset)} train, {len(val_dataset)} val, {len(test_dataset)} test\")\n",
        "print(f\"Device: {device}\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# Validate datasets before training\n",
        "validate_dataset(train_dataset, num_samples=5)\n",
        "validate_dataset(val_dataset, num_samples=5)\n",
        "validate_dataset(test_dataset, num_samples=5)\n",
        "\n",
        "# Evaluate model on test set before training\n",
        "print(\"\\nEvaluating model on test set before training...\")\n",
        "evaluation_results = evaluate_model_comprehensive(\n",
        "    model, test_loader, device, CONFIG['class_names']\n",
        ")\n",
        "\n",
        "# Execute training\n",
        "training_history = train_model()\n",
        "\n",
        "# Save the best model after training\n",
        "print(\"\\nSaving best model...\")\n",
        "best_checkpoint_path = save_model_checkpoint(\n",
        "    model, optimizer, scheduler,\n",
        "    epoch=len(training_history['train_losses']),\n",
        "    val_acc=training_history['best_val_acc'],\n",
        "    val_loss=min(training_history['val_losses']),\n",
        "    config=CONFIG,\n",
        "    filename=\"vgg16_skinlesion_best_model.pth\"\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining completed!\")\n",
        "print(f\"Best validation accuracy: {training_history['best_val_acc']:.2f}%\")\n",
        "print(f\"Total training time: {training_history['total_time']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9105d59",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c9105d59",
        "outputId": "673a8c51-d26a-470e-af20-cae7c0ad3ec6"
      },
      "outputs": [],
      "source": [
        "# Comprehensive model evaluation on test set\n",
        "print(\"\\nStarting Comprehensive Model Evaluation\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "evaluation_results = evaluate_model_comprehensive(\n",
        "    model, test_loader, device, CONFIG['class_names']\n",
        ")\n",
        "\n",
        "# Analyze misclassifications\n",
        "print(\"\\nAnalyzing Misclassifications...\")\n",
        "analyze_misclassifications(model, test_loader, device, CONFIG['class_names'])\n",
        "\n",
        "# Save complete results\n",
        "results_path = save_training_results(training_history, evaluation_results)\n",
        "\n",
        "# Export model for inference\n",
        "inference_model_path = export_model_for_inference(model)\n",
        "\n",
        "print(f\"\\nComplete Evaluation Finished!\")\n",
        "print(f\"Test Accuracy: {evaluation_results['accuracy']*100:.2f}%\")\n",
        "print(f\"Results saved to: {results_path}\")\n",
        "print(f\"Inference model: {inference_model_path}\")\n",
        "\n",
        "# Feature extraction example\n",
        "print(f\"\\nFeature Extraction Capability:\")\n",
        "print(\"The model can extract 512-dimensional features from the second-to-last layer.\")\n",
        "print(\"Use model.extract_features(images) for feature extraction.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b552ba02",
      "metadata": {
        "id": "b552ba02"
      },
      "source": [
        "## Training Complete - Summary and Next Steps\n",
        "\n",
        "### What We Accomplished:\n",
        "\n",
        "**Complete VGG16 Transfer Learning Pipeline**\n",
        "- Implemented medical-image optimized data preprocessing\n",
        "- Created balanced training with class weights for imbalanced data\n",
        "- Integrated with our GPU-optimized Azure pipeline\n",
        "- Added comprehensive evaluation metrics\n",
        "\n",
        "**Advanced Model Architecture**\n",
        "- VGG16 backbone with frozen feature layers\n",
        "- Custom classifier head optimized for 9-class skin lesion diagnosis\n",
        "- Feature extraction capability from second-to-last layer\n",
        "- Dropout regularization and early stopping\n",
        "\n",
        "**Production-Ready Training Infrastructure**\n",
        "- Automated checkpoint saving and loading\n",
        "- Real-time training visualization and monitoring\n",
        "- Comprehensive evaluation with confusion matrices\n",
        "- Export functionality for deployment\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "1. **Model Optimization**: Fine-tune hyperparameters based on results\n",
        "2. **Feature Analysis**: Use extracted features for additional ML models\n",
        "3. **Deployment**: Deploy model using Azure Container Apps\n",
        "4. **Clinical Validation**: Test on additional medical image datasets\n",
        "\n",
        "### Generated Files:\n",
        "- `../models/vgg16_skinlesion_best_model.pth` - Best trained model\n",
        "- `../models/vgg16_skinlesion_inference_*.pth` - Inference-ready model\n",
        "- `../results/training_results_*.pkl` - Complete training history and metrics"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cv_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
